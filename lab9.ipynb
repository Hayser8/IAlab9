{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f261ac8",
   "metadata": {},
   "source": [
    "## **Laboratorio 9**\n",
    "## **Julio García Salas - 22076**\n",
    "## **Sofía García - 22210**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14cbba4",
   "metadata": {},
   "source": [
    "# Modelos de Markov y Hidden Markov Models (HMM)\n",
    "\n",
    "## 1. Diferencia entre Modelos de Markov y Hidden Markov Models\n",
    "\n",
    "Un **Modelo de Markov (Markov Chain)** es un modelo probabilístico que describe una secuencia de posibles eventos en los que la probabilidad de cada evento depende únicamente del estado alcanzado en el evento anterior. Es decir, el proceso tiene la propiedad de **Markov**, también conocida como **memoria limitada**.\n",
    "\n",
    "- En un Modelo de Markov clásico, los **estados son observables**.\n",
    "- Se describe con una matriz de transición $A$ entre estados.\n",
    "\n",
    "Un **Modelo Oculto de Markov (Hidden Markov Model, HMM)** es una extensión del modelo de Markov donde el **estado real no es observable directamente** (es \"oculto\"), pero se puede inferir a partir de **observaciones** que dependen probabilísticamente de los estados ocultos.\n",
    "\n",
    "- En un HMM, se tiene:\n",
    "  - Una secuencia de **estados ocultos** $q_1, q_2, ..., q_T$\n",
    "  - Una secuencia de **observaciones** $o_1, o_2, ..., o_T$\n",
    "  - Probabilidades de emisión $B$ que definen la probabilidad de observar $o_t$ dado el estado oculto $q_t$\n",
    "\n",
    "## 2. ¿Qué son los factorial HMM?\n",
    "\n",
    "Los **Factorial Hidden Markov Models (FHMM)** son una generalización de los HMM tradicionales. En lugar de tener una única cadena de estados ocultos, un FHMM utiliza **varias cadenas ocultas que evolucionan en paralelo** y que juntas determinan la distribución de las observaciones.\n",
    "\n",
    "- Cada cadena de estados ocultos sigue un modelo de Markov independiente.\n",
    "- Las observaciones se generan a partir de la **combinación de múltiples cadenas ocultas**.\n",
    "\n",
    "Esto permite modelar **dependencias más complejas** entre los estados y las observaciones. Son útiles en contextos donde múltiples factores ocultos influyen en los datos observados (por ejemplo, reconocimiento de actividad humana, bioinformática, etc.).\n",
    "\n",
    "## 3. Algoritmo Forward-Backward para HMM\n",
    "\n",
    "El algoritmo **Forward-Backward** es una técnica de inferencia utilizada en HMMs para **calcular la probabilidad de una secuencia de observaciones** y para inferir la **probabilidad posterior de los estados ocultos** en cada tiempo dado.\n",
    "\n",
    "Se divide en dos fases:\n",
    "\n",
    "- **Forward ($\\alpha$) paso**: calcula de manera recursiva la probabilidad de la secuencia observada hasta el tiempo $t$ y de estar en un estado $i$ en ese momento:\n",
    "  \n",
    "  $$\n",
    "  \\alpha_t(i) = P(o_1, o_2, ..., o_t, q_t = i \\mid \\lambda)\n",
    "  $$\n",
    "\n",
    "- **Backward ($\\beta$) paso**: calcula la probabilidad de observar el resto de la secuencia desde $t+1$ en adelante, dado que el sistema está en el estado $i$ en el tiempo $t$:\n",
    "\n",
    "  $$\n",
    "  \\beta_t(i) = P(o_{t+1}, o_{t+2}, ..., o_T \\mid q_t = i, \\lambda)\n",
    "  $$\n",
    "\n",
    "- Finalmente, la probabilidad posterior de estar en un estado $i$ en el tiempo $t$ dado la secuencia completa de observaciones es:\n",
    "\n",
    "  $$\n",
    "  \\gamma_t(i) = \\frac{\\alpha_t(i) \\cdot \\beta_t(i)}{\\sum_{j=1}^N \\alpha_t(j) \\cdot \\beta_t(j)}\n",
    "  $$\n",
    "\n",
    "## 4. ¿Por qué es necesario el paso Backward?\n",
    "\n",
    "El paso **Backward** permite incorporar la información **futura** (las observaciones posteriores al tiempo $t$) para estimar correctamente la probabilidad de que el sistema haya estado en un determinado estado en ese tiempo.\n",
    "\n",
    "### Ejemplo:\n",
    "\n",
    "Supongamos que tenemos una observación inesperada al final de la secuencia que difícilmente pudo haber ocurrido en algunos estados anteriores. Sin el paso **Backward**, el algoritmo no tendría forma de ajustar las probabilidades anteriores con base en esa nueva información.\n",
    "\n",
    "- Por ejemplo, si en el tiempo $t=1$ se considera probable estar en el estado $S_1$, pero en el tiempo $t=3$ se observa un evento muy improbable desde $S_1$, el paso backward puede ajustar hacia abajo la probabilidad de $S_1$ en $t=1$.\n",
    "\n",
    "Esto se debe a que los **HMM son modelos generativos de toda la secuencia**, y no solo del presente. Sin el paso backward, solo se tendría una visión parcial (solo hacia adelante) y se perdería contexto importante que afecta la inferencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40594a9f",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ef0d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencia Generada: None\n",
      "Probabilidades Forward:\n",
      "None\n",
      "Probabilidades Backward:\n",
      "None\n",
      "Probabilidades de Estados:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class HMM:\n",
    "    def __init__(self, states, observations, initial_prob, transition_prob, emission_prob):\n",
    "        # Inicializar parámetros de HMM\n",
    "        self.states = states\n",
    "        self.observations = observations\n",
    "        self.initial_prob = initial_prob\n",
    "        self.transition_prob = transition_prob\n",
    "        self.emission_prob = emission_prob\n",
    "\n",
    "    def generate_sequence(self, length):\n",
    "        # Generar una secuencia de observaciones basada en el HMM\n",
    "        pass\n",
    "\n",
    "    def forward(self, observations):\n",
    "        # Implementar el paso hacia adelante del algoritmo hacia atrás-adelante\n",
    "        pass\n",
    "\n",
    "    def backward(self, observations):\n",
    "        # Implementar el paso hacia atrás del algoritmo hacia atrás-adelante\n",
    "        pass\n",
    "\n",
    "    def compute_state_probabilities(self, observations):\n",
    "        # Combinar probabilidades hacia adelante y hacia atrás para calcular probabilidades de estado\n",
    "        pass\n",
    "\n",
    "\n",
    "# Uso y datos\n",
    "states = ['Sunny', 'Rainy']\n",
    "observations = ['Sunny', 'Sunny', 'Rainy', 'Rainy']\n",
    "initial_prob = {'Sunny': 0.5, 'Rainy': 0.5}\n",
    "transition_prob = {\n",
    "    'Sunny': {'Sunny': 0.8, 'Rainy': 0.2},\n",
    "    'Rainy': {'Sunny': 0.4, 'Rainy': 0.6}\n",
    "}\n",
    "emission_prob = {\n",
    "    'Sunny': {'Sunny': 0.8, 'Rainy': 0.2},\n",
    "    'Rainy': {'Sunny': 0.3, 'Rainy': 0.7}\n",
    "}\n",
    "\n",
    "hmm = HMM(states, observations, initial_prob, transition_prob, emission_prob)\n",
    "\n",
    "# Generar una secuencia de observaciones.\n",
    "obs_sequence = hmm.generate_sequence(5)\n",
    "print(\"Secuencia Generada:\", obs_sequence)\n",
    "\n",
    "# Cálculo de probabilidades Forward\n",
    "forward_probs = hmm.forward(observations)\n",
    "print(\"Probabilidades Forward:\")\n",
    "print(forward_probs)\n",
    "\n",
    "# Cálculo de probabilidades Backward\n",
    "backward_probs = hmm.backward(observations)\n",
    "print(\"Probabilidades Backward:\")\n",
    "print(backward_probs)\n",
    "\n",
    "# Calcular probabilidades de estado\n",
    "state_probs = hmm.compute_state_probabilities(observations)\n",
    "print(\"Probabilidades de Estados:\")\n",
    "print(state_probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
